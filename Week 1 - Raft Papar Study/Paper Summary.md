# Summary of <In Search of an Understandable Consensus Algorithm (Extended Version)>

## Abstract

* Raft 는 복제된 로그를 관리하기 위한 합의 알고리즘.
* 기존에 많이 쓰이던 Paxos 만큼 효율적이고 동일한 결과를 내지만, 내부 구조가 달라서 이해하기 훨씬 쉬움.
* 이해하기 쉽게 만들기 위해 Raft 는 고려해야 할 여러 시나리오를 최대한 줄일 수 있는 방향, 즉 최대한 간단한 방식으로 리더 선출, 로그 복제, 안전성 보장 등을 해결하고자 함.

## 1. Introduction

- 합의 알고리즘은 분산 시스템에서 일부 노드에 장애가 생기더라도 시스템이 정상적으로 작동할 수 있도록 한다.
- 기존에 있던 Paxos 알고리즘은 이해하기가 너무너무너무 어려워서 공부하기도 힘들고 실제 구현체를 제대로 만들기도 쉽지 않았다.
- Raft 의 목표는 분산 시스템 구현을 좀 더 용이하게 할 수 있는 토대를 마련함과 동시에 이해하기 쉬운 합의 알고리즘이 되는 것.
- 따라서 기존 다른 분산 알고리즘과 비슷하게 동작하면서도 다른 점이 몇가지 있다.
  - **Strong leader**: 다른 분산 알고리즘보다 강한 리더십을 사용한다. 예를 들어, 로그 엔트리는 항상 리더에서 팔로워 방향으로만 전달된다.
  - **Leader election**: 리더 선출을 위해 랜덤 타이머를 사용한다. 이렇게 하면 간단한 로직만으로 목표를 달성할 수 있다.
  - **Membership changes**: Joint consensus 방식을 도입해서 멤버십 정보가 바뀌더라도 클러스터가 정상적으로 작동할 수 있게 만들었다.

## 2. Replicated state machines

- 합의 알고리즘은 보통 복제된 상태 기계를 다루면서 생긴다.
- 복제된 상태 기계 방식에서는 각 서버가 동일한 상태에 대한 복사본을 갖고, 특정 서버가 죽더라도 시스템은 계속해서 동작한다.
- 복제된 상태 기계는 일반적으로 복제 로그를 구현하여 만들어진다.
  - 모든 서버가 동일한 로그를 같은 순서로 적용하여 똑같은 상태를 유지하는 것
  - 상태 기계는 결론적이므로 각 서버는 동일한 상태와 동일한 출력 순서를 갖는다.

- 합의 알고리즘의 역할은 로그의 일관성을 유지하는 것.
- 실제 시스템을 위한 합의 알고리즘은 보통 아래 성질을 가진다.
  - 네트워크 지연, 파티션, 패킷 손실, 중복, 재정렬 등을 포함하여 비잔틴이 아닌 모든 조건에서 안전성을 보장함
  - 대다수의 서버가 살아있으며 서버와 서버끼리, 서버와 클라이언트끼리 통신이 가능하다면 시스템은 제대로 동작함
    - 즉, 정족수 이하의 서버가 죽더라도 전체 시스템은 멀쩡함

  - 로그 일관성을 위해 타이밍에 의존하지 않음: 최악의 경우 가용성 문제 발생 가능
  - 보통 몇몇 느린 서버가 전체 시스템에 영향을 주지 않도록 대다수의 서버가 응답을 했다면 해당 커맨드는 처리 완료된 것으로 간주함


## 3. What's wrong with Paxos?

* Paxos 알고리즘에서는 single-decree Paxos 로 시작하여 이를 multi-Paxos 로 확장한다.
* 첫 번째 문제는 Paxos 가 이해하기 정말정말 어려운 알고리즘인 것.
  * 저자는 Paxos 가 그렇게까지 이해하기 어려운 이유는 single-decree subset 에 기반하고 있기 때문일 것으로 생각한다.
* 두 번째 문제는 실제 구현체를 만들기에 그닥 좋은 기반을 제공하지 못한다는 점.
* 결론적으로 실제 구현된 시스템은 Paxos 의 일부만 흉내내는 정도에 그치게 되며 결국에는 Paxos 와 다른 무언가가 탄생하곤 했다.
* 따라서 Paxos 는 실제 시스템 구축 용도로도 교육용으로도 적합하지 못한 알고리즘.

## 4. Designing for understandability

* Raft 의 1순위 목표는 이해하기 쉬운 합의 알고리즘을 만드는 것
* 2순위 목표는 시스템을 설계하는 실용적인 기반을 제공하는 것
* 위 목표를 달성하기 위해 Raft 에서는 이런 기법을 썼다
  * Divide and conquer
  * 고려해야 할 상태 수를 줄이고 최대한 단순화

## 5. The Raft consensus algorithm

* Raft 는 복제된 로그를 관리하기 위한 알고리즘
* Raft 에서는 합의를 이끌어내기 위해 먼저 리더를 선출한 다음, 리더에게 복제 로그를 관리하는 모든 책임을 맡긴다.
  * 리더가 있기 때문에 로그 복제 과정이 간결해진다. (앞에서 본 `고려해야 할 상태 수를 줄이고 최대한 단순화`의 예시 중 하나)
* Raft 는 리더-팔로워 방식을 사용하여 합의 알고리즘에서 다뤄야 할 문제를 크게 3가지로 나눴다. (앞에서 본 divide and conquer)
  * **Leader election**: 지금 존재하는 리더에 장애가 생겼을 때 반드시 새로운 리더가 선택되어야 함
  * **Log replication**: 리더는 반드시 클라이언트로부터 로그 엔트리를 받고 클러스터 내에서 이를 복제해야 하며, 다른 로그도 '동의'하게 만들어야 함.
  * **Safety**: Raft 의 안전성은 State Machine Safety 로부터 나오며, 이 성질은 리더 선출 메커니즘에 제약 조건을 추가함으로써 보장할 수 있음.

### 5.1 Raft basics

* Raft 클러스터는 일반적으로 5개 노드로 구성 (정족수)
* 각 서버는 다음 3가지 역할 중 하나를 함: 리더, 팔로워, 후보
  * 정상적인 상황에서 항상 1개의 리더만 존재
  * 팔로워는 수동적: 리더나 후보의 요청을 받아들이기만 하고 스스로 요청을 만들어내지 않음.
  * 리더가 모든 클라이언트 요청을 처리
    * 만약 클라이언트가 팔로워에게 요청을 보냈다면 팔로워는 리더로 그 요청을 리다이렉트
  * 후보는 새로운 리더 선출 과정을 위한 역할
* Raft 에서는 시간을 임의의 길이로 쪼개어 `term` 으로 관리한다. Logical clock 역할을 함.
  * 새로운 `term` 의 시작 시점: 리더 선출이 시작될 때
  * `term` 이 종료되는 시점
    * 새 term 이 시작되기 전
    * 투표에서 리더를 선출하지 못했을 때
  * 즉, 하나의 `term` 에는 **최대 1개** (!= 정확히 1개)의 리더가 존재한다.
* 각 서버는 자신의 `current term` 을 갖고, 다른 노드와 통신할 때 항상 이걸 주고받는다.
  * 다른 서버보다 작은 term 을 갖고 있는 서버는 그보다 큰 값으로 업데이트함
  * 리더가 자신의 term 이 다른 서버보다 작다는걸 인지하면 즉시 팔로워 역할로 돌아감.
  * 자신의 term 보다 작은 값을 가진 요청이 들어왔다면 해당 요청은 무시함
* Raft 에서 서버끼리의 통신은 RPC 를 통해 이루어 짐
  * 만약 시간 내에 응답을 받지 못했다면 계속 재전송
  * RPC 전송은 병렬 처리

### 5.2 Leader election

* 리더 선출을 트리거하기 위해 heartbeart 메커니즘을 사용함.

  * 리더는 자신이 살아있음을 모든 팔로워에게 일정 주기로 전송: AppendEntries RPC
    * 이 때 전송하는 데이터에는 로그 엔트리가 비어있음.

* **election timeout**: 팔로워가 이 시간동안 리더로부터 heartbeat 를 받지 못하면 리더에게 장애가 생긴 것으로 판단하고 투표를 시작함.

  * Raft 에서는 가능한 1개의 서버만 election timeout 되도록, 타임아웃 값을 랜덤으로 정해서 split vote 를 최대한 방지함.
    * 랜덤으로 정하지 않으면 자꾸 코너케이스가 생겨서 알고리즘이 복잡해지고 이해하기도 어려워짐.

* 리더 선출 과정

  1. 특정 팔로워가 자신의 election timeout 이 지나도록 리더의 heartbeat 를 받지 못함

  2. 팔로워가 자신의 term 값을 올리고 후보 역할이 됨 - 이 후보를 C₁ 이라고 하자.

  3. C₁ 이 자기 자신에게 1표를 던지고 RequestVote RPC 를 다른 서버에게 발행

  4. 아래 3가지 상황이 발생하기 전까지 C₁ 은 후보 상태로 대기

  5. 5-1. **C₁ 이 리더로 결정됨**

     - 동일한 term 에 대해 대다수의 서버가 C₁ 이 리더임에 동의한 경우.
       - 각 서버는 하나의 term 에 대해서 최대 1개의 후보에게만 찬성표를 던질 수 있음.
         - 찬성표를 받을 후보를 고르는 기준: FIFO.
     - 6. 리더가 된 C₁ 은 다른 모든 서버에게 heartbeat 을 보냄.

     5-2. **투표가 끝나기 전, C₁ 이 아닌 다른 서버로부터 heartbeat 를 받음**

     - 6-1. Heartbeat 에 있는 term 이 C₁ 의 것보다 크다면: Heartbeat 를 보낸 노드가 리더임을 인정하고 C₁ 은 팔로워 역할이 됨.
     - 6-2. Heartbeat 에 있는 term 이 C₁ 의 것보다 작다면: 무시. 다시 4단계로 돌아감.

     5-3. **Split vote: 리더가 뽑히지 못한 채로 시간이 흐름 **

     * 아무도 과반수의 찬성표를 받지 못함. 주로 여러 팔로워가 동시에 후보 역할이 되었을 때 발생.
     * 6. 각 서버는 새로운 선거를 시작하기 위해 자신의 term 값을 올리고 RequestVote RPC 를 보냄.
       7. 이 때에도 선거가 시작되면 자신의 election timeout 값을 랜덤으로 픽.

     * 이론상 이 상황이 영원히 유지될 수도 있음.

* 리더 선출이 되기 위한 조건인 '과반수의 찬성표'에서 '과반수'에는 C₁ 자기자신도 포함된다.

### 5.3 Log replication

* 리더의 역할
  * 클라이언트의 요청 처리
  * 로그 복제 요청 == 다른 서버가 로그 복제할 수 있도록 AppendEntries RPC 를 병렬 전송하는 것.
  * 로그 복제 요청에 대한 ACK 처리
  * 로그 복제 요청 재시도
    * 과반수의 팔로워가 로그 복제에 대한 ACK를 보냈다면 리더는 해당 로그 엔트리를 자신에게 적용하고, 클라이언트에게 응답함.
      * 그래야 느린 노드가 전체 시스템에 영향을 주지 않으니까.
    * 클라이언트에게 응답한 것과 무관하게, 나머지 서버가 모두 ACK를 보낼 때까지 계속해서 재시도.
* 과반수 이상이 복제에 성공한 로그 엔트리는 `committed` 되었다고 표현함.
  * 리더는 가장 최근에 커밋 된 로그 엔트리의 인덱스를 보유하고, AppendEntries RPC 를 보낼 때마다 같이 보낸다.
    * 그래서 팔로워가 엔트리를 자신의 상태 머신에 적용할 수 있도록
  * ⚠️ 헷갈렸던 부분 - `committed된 로그 != 과반수 이상이 apply 한 로그`.
    * 커밋 된 로그는 과반수 이상이 **복제**한 로그이지, 자신의 상태에 **적용**한 로그가 아니다.
    * 팔로워는 리더의 AppendEntries RPC 에 들어온 마지막으로 커밋 된 엔트리 인덱스를 본 다음에야 해당 엔트리를 자기 자신에게 적용한다.
* Log Matching Property 를 구성하는 2가지 성질 - 동일한 로그를 찾아서...
  * 서로 다른 로그의 두 엔트리가 동일한 인덱스와 term 을 가진다면 그 둘은 동일한 커맨드를 담고 있다.
  * 서로 다른 로그의 두 엔트리가 동일한 인덱스와 term 을 가진다면 그 전에 있는 다른 모든 엔트리에 대해 두 로그는 동일한 엔트리를 가진다.
    * AppendEntries 할 때 일관성 체크를 함으로써 이 성질을 보장할 수 있다.
      * 일관성 체크 HOW TO?
        * 리더는 AppendEntries RPC를 보낼 때 이번 로그 엔트리 바로 앞에 있는 엔트리의 인덱스와 term 을 같이 보낸다.
        * 팔로워는 AppendEntries RPC를 받아서 '바로 앞에 있는 엔트리'와 동일한 인덱스, term 을 갖는 엔트리를 자신의 로그에서 찾지 못하면 그 RPC 는 무시한다.
      * 최초 상태인 비어있는 로그는 그 자체로 Log Matching Property 를 만족하고, 매번 새 엔트리가 들어올 때마다 위 과정 - 일관성 체크 - 를 하기 때문에 Log Matching Property 가 성립한다.
* 데이터 일관성이 깨지는 경우 중 하나: 로그 복제가 완료되기 전에 죽을 때. 이 현상은 여러 term 에 걸쳐서 발생할 수 있음.
  * 리더는 이 현상을 해결하기 위해 팔로워에서 문제가 되는 로그를 올바른 로그로 덮어쓰도록 만듦.
    * 아니 이래도 됩니까? --> 예. 대신 _안전하게_ 덮어쓰기 위해 아래 과정이 필요함.
      * 리더는 각 팔로워별로 `nextIndex` 값을 관리: 리더가 다음번 RPC에서 팔로워에게 보낼 엔트리의 인덱스. (즉, 아직 팔로워에게 전송은 안 된 엔트리의 인덱스)
      * 노드는 리더가 되면 모든 팔로워의 nextIndex 값을 자기가 갖고 있는 마지막 로그 엔트리의 바로 다음 값으로 업데이트.
    * 그러면 팔로워가 다음번 AppendEntries RPC 받았을 때 이렇게 됨.
      * Consistent 한 팔로워라면: 그대로 log append 하고 끝.
      * Inconsistent 한 팔로워라면: 이번 RPC 는 `reject`. --> Rejection 을 받으면 리더는 `nextIndex` 값을 감소시켜서 AppendEntries RPC 를 계속 보낸다. 언제까지? 성공할 때까지.
        * 필요하다면 AppendEntries RPC 가 reject 되는 횟수를 줄이는 방향으로 최적화 할 수도 있음.

### 5.4 Safety

지금까지 설명한것만으로는 leader completeness 를 보장할 수 없다. 몇가지 제약사항을 더 추가해보자.

#### 5.4.1 Election restriction

- 모든 합의 알고리즘이 다 그런것은 아니지만 Raft 에서는 로그 전송 방향이 항상 무슨 일이 있어도 리더 --> 팔로워 방향이 된다.
- 후보 노드가 up-to-date 한 노드가 아니라면 나머지 노드는 그 노드에게 표를 던지지 않는다.
  - Up-to-date 한 노드를 알아내는 방법: 마지막 로그의 term, index 를 비교.

#### 5.4.2 Committing entries from previous terms

- 리더가 갖고 있는 로그 중, 현재 term이 아닌 과거 term에 해당하는 로그가 commit 되기 전에 리더가 죽으면, 일관성이 깨질 수 있다.
- 이를 방지하기 위해 리더는 현재 term에 해당하는 엔트리에 대해서만 복제하려고 시도해야한다.

#### 5.4.3 Safety argument

* (자세한 과정은 생략) Leader Completeness 의 역이 참이라고 가정했을 때 모순이 생기는 것을 보이며 이 성질이 성립함을 증명함.

### 5.5 Follwer and candidate crashes

- 리더가 아닌 팔로워나 후보 노드에 장애가 생겼을 때 해결 방법: 원래 그들이 처리해야 하는 RPC를 무한히 재전송한다.
  - Raft 의 RPC 는 idempotent 하기 때문에 계속 보내도 괜찮음.!

### 5.6 Timing and availability

- Raft 는 강한 일관성 유지를 목표로 하기 때문에 어떠한 이벤트가 빠르거나 느리게 일어났다고 해서 _잘못된_ 결과를 줘서는 안 됨.
- 따라서 raft 에서 쓰이는 시간 관련 변수값은 이렇게 정의되어야 함.
  - broadcastTime << electionTimeout << MTBF
  - BroadcastTime: 한 서버가 클러스터 내의 다른 서버에게 병렬로 RPC 를 보내고 응답을 받기까지 걸리는 평균 시간
  - MTBF Mean Time Between Failure: 한 서버가 살아있는 평균적인 시간
  - 여기서 우리가 직접 정할 수 있는 값은 election timeout 값임: 보통 10ms - 500ms 사이로 잡는다.

## 6. Cluster membership changes

- 분산 시스템에서의 클러스터 멤버십, 즉 클러스터 내에 어떤 노드가 있는지를 관리하고 안전하게 업데이트하는 것도 raft가 해야 할 일 중 하나.
- Reconfiguration 시의 4가지 이슈와 해결책
  - Safety
    - 그냥 전환해버리면 disjoint majorities 상황이 발생할 수 있음. (마치 ES의 split brain 같은거)
      - Raft 에서는 joint consensus 를 도입해 이 문제를 해결한다.
    - Joint consensus: 안전한 클러스터 멤버십 업데이트를 위해 old/new 설정(각각 C_old, C_new 라고 하자)을 합친 설정.
      - 로그 엔트리는 C_old, C_new 의 노드 모두에게 복제됨
      - C_old, C_new 둘 중 어느 쪽에서든 새 리더가 선출될 수 있음.
      - 합의가 필요할 때 C_old, C_new 각각에서 과반수 이상의 찬성 혹은 ok 응답을 받아야 합의 완료.
  - Empty new server
    - 그동안의 로그 데이터를 갖고 있지 않은 empty 노드가 C_new 에 추가되었을 때, 해당 노드는 선거에 영향을 미치지 않도록 non-voting 상태가 됨.
    - Non-voting 멤버는
      - 로그 복제를 수행하고, 완료되기 전까지는 majority 계산 할 때 고려되지 않음.
      - 로그 복제를 끝까지 완료했다면 선거에 참여함.
  - C_old 에만 존재하는 노드가 리더가 될 수 있음
    - 이렇게 뽑힌 리더는 C_new 를 커밋한 후 자신은 팔로워 상태로 돌아감으로써 C_new 의 노드들에게 주도권을 넘겨줌
  - C_old 에만 존재하는 노드가 리더 선출을 방해할 수 있음
    - 상황: C_new 가 커밋되면 C_old 에만 존재하는 노드들은 더이상 리더가 보내는 heartbeat 를 받지 못함 --> election timeout 발동 --> 계속해서 반복하며 무의미한 선거가 시작됨
    - 해결: 노드들은 현재 리더가 존재한다고 생각되면 RequestVote RPC 를 무시함.
      - 즉, 랜덤값으로 정해지는 election timeout 중 최소값보다 짧은 시간 이내에 RequestVote RPC 를 또 수신했다면 그건 무시함.

## 7. Log compaction

- 현실 세계에서 로그가 무한정 늘어날 수 없음. 또한 그래서는 안 됨. Availability 를 위해서. 따라서 로그 압축 기술이 필요함.

- Raft 에서는 로그 압축을 위해 스냅샷을 만든다.

  - 왜 스냅샷 방법을 채택했나? 심플해서.

  - 스냅샷은 각 서버별로 알아서 찍는다.

    - 이렇게 해도 raft 의 철학을 위반하는 것은 아니며, 이렇게 하지 않으면(= 리더만 스냅샷을 생성하게 되면) 오히려 성능에 영향이 갈 수 있고 심플함을 챙기기 어려워짐.

    - 서버별로 스냅샷 찍는 방법

      1. 다음 데이터를 포함한 스냅샷을 만든다: index, term, state, config

      2. 자신의 다른 logs 와 나머지 스냅샷을 지운다.

  - 각 서버별로 스냅샷을 찍음에도 불구하고, 리더가 스냅샷을 전파해야 하는 경우: 유난히 느린 팔로워가 있거나, 빈 서버가 클러스터에 새롭게 참여하는 경우.

    - 이 때 리더는 InstallSnapshot RPC 를 보내고 팔로워는 이걸 받아서 처리함.

  - 스냅샷은 언제 찍나? 로그 사이즈가 특정값에 도달했을 때.

  - 스냅샷 과정이 너무 길어지는 것을 막기 위해 copy-on-write 기술을 사용한다.

## 8. Client interaction

* Raft 는 강한 일관성을 보장한다.
* 클라이언트가 팔로워가 아닌 리더에게 요청을 보내는 방법: 클라이언트가 리더를 찾을 때까지 알아서 재시도 한다.
  * ⚠️ 헷갈렸던 부분 - Read 요청이라도 팔로워에게 요청이 들어가면 리다이렉트를 한다.
* Write 요청에 대해 강한 일관성을 보장하기 위해, raft 에서는 클라이언트별 커맨드 인덱스를 따로 관리한다.
  * 클라이언트: 매 커맨드별로 unique 한 시리얼 번호를 매김
  * 서버: 클라이언트별 마지막 시리얼 번호를 갖고 다니면서, 그 이전 번호에 해당하는 요청이 들어오면 반영하지 않는다.
* Read 요청에 대해 강한 일관성을 보장하기 위해, raft는 아래 2가지를 지켜야 한다.
  * 리더는 반드시 어떤 항목이 커밋되었는지에 대한 최신 정보를 갖고 있어야 함.
    * Leader completeness 속성이 이를 보장하지만, term 의 시작 시점에는 해당 term 의 엔트리가 없기 때문에 비어있는 **no-op** 엔트리를 커밋한다.
  * 리더는 read-only 요청을 처리하기 전에 자신이 현재 리더가 맞는지 확인하기 위해 heartbeat 메세지를 교환하여 클러스터 내 과반 이상의 노드로부터 확인을 받음.

(섹션 9~10은 raft의 성능, 유용함에 대한 것으로, 생략)

## 11. Conclusion

* 실제로 구현 가능한 알고리즘이어야 가치가 있다. Raft 에서는 이를 달성했고 Paxos 보다 나음 ^^.

